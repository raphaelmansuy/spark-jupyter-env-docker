{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b58a17b-3b47-4ccd-b1e7-2474e49caf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                 \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  ;\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.postgresql:postgresql:42.3.6`\n",
    "\n",
    "import $ivy.`org.apache.spark::spark-sql:3.0.0`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d322953-2346-4772-bab6-8ebe23bcdca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/06/09 15:05:18 INFO SparkContext: Running Spark version 3.0.0\n",
      "22/06/09 15:05:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/09 15:05:18 INFO ResourceUtils: ==============================================================\n",
      "22/06/09 15:05:18 INFO ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "22/06/09 15:05:18 INFO ResourceUtils: ==============================================================\n",
      "22/06/09 15:05:18 INFO SparkContext: Submitted application: scala-spark-notebook\n",
      "22/06/09 15:05:18 INFO SecurityManager: Changing view acls to: root\n",
      "22/06/09 15:05:18 INFO SecurityManager: Changing modify acls to: root\n",
      "22/06/09 15:05:18 INFO SecurityManager: Changing view acls groups to: \n",
      "22/06/09 15:05:18 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/06/09 15:05:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "22/06/09 15:05:19 INFO Utils: Successfully started service 'sparkDriver' on port 36365.\n",
      "22/06/09 15:05:19 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/06/09 15:05:19 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/06/09 15:05:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/06/09 15:05:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/06/09 15:05:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/06/09 15:05:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9ee19dbe-e64c-4e99-9fd5-2a858e47c69a\n",
      "22/06/09 15:05:19 INFO MemoryStore: MemoryStore started with capacity 1111.8 MiB\n",
      "22/06/09 15:05:19 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/06/09 15:05:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/06/09 15:05:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://227281bc4555:4040\n",
      "22/06/09 15:05:19 INFO SparkContext: Added JAR jars/postgresql-42.3.6.jar at spark://227281bc4555:36365/jars/postgresql-42.3.6.jar with timestamp 1654787119418\n",
      "22/06/09 15:05:19 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "22/06/09 15:05:19 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.3:7077 after 23 ms (0 ms spent in bootstraps)\n",
      "22/06/09 15:05:19 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220609150519-0018\n",
      "22/06/09 15:05:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220609150519-0018/0 on worker-20220609131701-172.19.0.5-44333 (172.19.0.5:44333) with 1 core(s)\n",
      "22/06/09 15:05:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20220609150519-0018/0 on hostPort 172.19.0.5:44333 with 1 core(s), 512.0 MiB RAM\n",
      "22/06/09 15:05:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220609150519-0018/1 on worker-20220609131701-172.19.0.6-42117 (172.19.0.6:42117) with 1 core(s)\n",
      "22/06/09 15:05:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20220609150519-0018/1 on hostPort 172.19.0.6:42117 with 1 core(s), 512.0 MiB RAM\n",
      "22/06/09 15:05:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36379.\n",
      "22/06/09 15:05:19 INFO NettyBlockTransferService: Server created on 227281bc4555:36379\n",
      "22/06/09 15:05:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/06/09 15:05:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 227281bc4555, 36379, None)\n",
      "22/06/09 15:05:19 INFO BlockManagerMasterEndpoint: Registering block manager 227281bc4555:36379 with 1111.8 MiB RAM, BlockManagerId(driver, 227281bc4555, 36379, None)\n",
      "22/06/09 15:05:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 227281bc4555, 36379, None)\n",
      "22/06/09 15:05:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220609150519-0018/0 is now RUNNING\n",
      "22/06/09 15:05:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220609150519-0018/1 is now RUNNING\n",
      "22/06/09 15:05:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 227281bc4555, 36379, None)\n",
      "22/06/09 15:05:20 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@133205dd\n",
       "\u001b[36mres1_2\u001b[39m: \u001b[32mClass\u001b[39m[\u001b[32m?0\u001b[39m] = class org.postgresql.Driver\n",
       "\u001b[36mjdbcHostname\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"db\"\u001b[39m\n",
       "\u001b[36mjdbcPort\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m5432\u001b[39m\n",
       "\u001b[36mjdbcDatabase\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"postgres\"\u001b[39m\n",
       "\u001b[36mjdbcUsername\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"postgres\"\u001b[39m\n",
       "\u001b[36mjdbcPassword\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"postgres\"\u001b[39m\n",
       "\u001b[36mjdbcUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"jdbc:postgresql://db:5432/postgres\"\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\n",
       "\u001b[36mconnectionProperties\u001b[39m: \u001b[32mProperties\u001b[39m = {user=postgres, password=postgres, driver=org.postgresql.Driver}\n",
       "\u001b[36mres1_11\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres1_12\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres1_13\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.DriverManager\n",
       "\u001b[39m\n",
       "\u001b[36mconnection\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mConnection\u001b[39m = org.postgresql.jdbc.PgConnection@e2188f5\n",
       "\u001b[36mres1_16\u001b[39m: \u001b[32mBoolean\u001b[39m = false"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = SparkSession.\n",
    "            builder().\n",
    "            appName(\"scala-spark-notebook\").\n",
    "            master(\"spark://spark-master:7077\").\n",
    "            config(\"spark.jars\", \"jars/postgresql-42.3.6.jar\").\n",
    "            config(\"spark.executor.memory\", \"512m\").\n",
    "            getOrCreate()\n",
    "\n",
    "Class.forName(\"org.postgresql.Driver\")\n",
    "\n",
    "\n",
    "val jdbcHostname = \"db\"\n",
    "val jdbcPort = 5432\n",
    "val jdbcDatabase = \"postgres\"\n",
    "val jdbcUsername = \"postgres\"\n",
    "val jdbcPassword = \"postgres\"\n",
    "\n",
    "// Create the JDBC URL without passing in the user and password parameters.\n",
    "val jdbcUrl = s\"jdbc:postgresql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}\"\n",
    "\n",
    "// Create a Properties() object to hold the parameters.\n",
    "import java.util.Properties\n",
    "val connectionProperties = new Properties()\n",
    "\n",
    "connectionProperties.put(\"user\", s\"${jdbcUsername}\")\n",
    "connectionProperties.put(\"password\", s\"${jdbcPassword}\")\n",
    "connectionProperties.put(\"driver\",\"org.postgresql.Driver\")\n",
    "\n",
    "import java.sql.DriverManager\n",
    "val connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword)\n",
    "connection.isClosed()            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7436a8ed-3788-42f0-bdb8-6019fce011e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 15:05:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/workspace/spark-warehouse').\n",
      "22/06/09 15:05:21 INFO SharedState: Warehouse path is 'file:/opt/workspace/spark-warehouse'.\n",
      "22/06/09 15:05:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/06/09 15:05:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.5:45668) with ID 0\n",
      "22/06/09 15:05:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.6:36824) with ID 1\n",
      "22/06/09 15:05:21 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.5:38187 with 93.3 MiB RAM, BlockManagerId(0, 172.19.0.5, 38187, None)\n",
      "22/06/09 15:05:21 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.6:41663 with 93.3 MiB RAM, BlockManagerId(1, 172.19.0.6, 41663, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, affiliation_key: string ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = \n",
    "  spark\n",
    "    .read\n",
    "    .jdbc(jdbcUrl,\n",
    "         \"affiliations\", \n",
    "         connectionProperties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0b131d-6bfe-4e9d-abc9-680db081dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 15:05:25 INFO CodeGenerator: Code generated in 323.911208 ms\n",
      "22/06/09 15:05:25 INFO SparkContext: Starting job: show at cmd3.sc:1\n",
      "22/06/09 15:05:25 INFO DAGScheduler: Got job 0 (show at cmd3.sc:1) with 1 output partitions\n",
      "22/06/09 15:05:25 INFO DAGScheduler: Final stage: ResultStage 0 (show at cmd3.sc:1)\n",
      "22/06/09 15:05:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/06/09 15:05:25 INFO DAGScheduler: Missing parents: List()\n",
      "22/06/09 15:05:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at show at cmd3.sc:1), which has no missing parents\n",
      "22/06/09 15:05:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.9 KiB, free 1111.8 MiB)\n",
      "22/06/09 15:05:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 1111.8 MiB)\n",
      "22/06/09 15:05:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 227281bc4555:36379 (size: 4.9 KiB, free: 1111.8 MiB)\n",
      "22/06/09 15:05:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200\n",
      "22/06/09 15:05:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at cmd3.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "22/06/09 15:05:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "22/06/09 15:05:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.19.0.5, executor 0, partition 0, PROCESS_LOCAL, 7175 bytes)\n",
      "22/06/09 15:05:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.0.5:38187 (size: 4.9 KiB, free: 93.3 MiB)\n",
      "22/06/09 15:05:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1328 ms on 172.19.0.5 (executor 0) (1/1)\n",
      "22/06/09 15:05:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/06/09 15:05:26 INFO DAGScheduler: ResultStage 0 (show at cmd3.sc:1) finished in 1.544 s\n",
      "22/06/09 15:05:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/06/09 15:05:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/06/09 15:05:26 INFO DAGScheduler: Job 0 finished: show at cmd3.sc:1, took 1.631063 s\n",
      "22/06/09 15:05:26 INFO CodeGenerator: Code generated in 18.624584 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------------+------------+\n",
      "| id|affiliation_key|affiliation_type|publisher_id|\n",
      "+---+---------------+----------------+------------+\n",
      "|  1|      l.mlb.com|          league|           1|\n",
      "|  2|       15007000|           sport|           1|\n",
      "|  3|      l.nfl.com|          league|           1|\n",
      "|  4|       15003000|           sport|           1|\n",
      "|  5|          c.afc|      conference|           1|\n",
      "|  6|      d.afceast|        division|           1|\n",
      "|  7|     d.afcnorth|        division|           1|\n",
      "|  8|     d.afcsouth|        division|           1|\n",
      "|  9|      d.afcwest|        division|           1|\n",
      "| 10|          c.nfc|      conference|           1|\n",
      "| 11|      d.nfceast|        division|           1|\n",
      "| 12|     d.nfcnorth|        division|           1|\n",
      "| 13|     d.nfcsouth|        division|           1|\n",
      "| 14|      d.nfcwest|        division|           1|\n",
      "| 15|      l.nba.com|          league|           1|\n",
      "| 16|       15008000|           sport|           1|\n",
      "| 17|      c.eastern|      conference|           1|\n",
      "| 18|     d.atlantic|        division|           1|\n",
      "| 19|      d.central|        division|           1|\n",
      "| 20|    d.southeast|        division|           1|\n",
      "+---+---------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3834487c-fd93-4945-97b7-91d3ac66a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 15:05:27 INFO CodeGenerator: Code generated in 11.180625 ms\n",
      "22/06/09 15:05:27 INFO CodeGenerator: Code generated in 9.84825 ms\n",
      "22/06/09 15:05:27 INFO SparkContext: Starting job: count at cmd4.sc:1\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Registering RDD 5 (count at cmd4.sc:1) as input to shuffle 0\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Got job 1 (count at cmd4.sc:1) with 1 output partitions\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Final stage: ResultStage 2 (count at cmd4.sc:1)\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at cmd4.sc:1), which has no missing parents\n",
      "22/06/09 15:05:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.9 KiB, free 1111.8 MiB)\n",
      "22/06/09 15:05:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 1111.8 MiB)\n",
      "22/06/09 15:05:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 227281bc4555:36379 (size: 6.2 KiB, free: 1111.8 MiB)\n",
      "22/06/09 15:05:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at cmd4.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "22/06/09 15:05:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "22/06/09 15:05:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 172.19.0.5, executor 0, partition 0, PROCESS_LOCAL, 7164 bytes)\n",
      "22/06/09 15:05:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.0.5:38187 (size: 6.2 KiB, free: 93.3 MiB)\n",
      "22/06/09 15:05:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 198 ms on 172.19.0.5 (executor 0) (1/1)\n",
      "22/06/09 15:05:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/06/09 15:05:27 INFO DAGScheduler: ShuffleMapStage 1 (count at cmd4.sc:1) finished in 0.230 s\n",
      "22/06/09 15:05:27 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/06/09 15:05:27 INFO DAGScheduler: running: Set()\n",
      "22/06/09 15:05:27 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
      "22/06/09 15:05:27 INFO DAGScheduler: failed: Set()\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at count at cmd4.sc:1), which has no missing parents\n",
      "22/06/09 15:05:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 1111.8 MiB)\n",
      "22/06/09 15:05:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1111.8 MiB)\n",
      "22/06/09 15:05:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 227281bc4555:36379 (size: 5.0 KiB, free: 1111.8 MiB)\n",
      "22/06/09 15:05:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at count at cmd4.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "22/06/09 15:05:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\n",
      "22/06/09 15:05:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 172.19.0.5, executor 0, partition 0, NODE_LOCAL, 7329 bytes)\n",
      "22/06/09 15:05:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.0.5:38187 (size: 5.0 KiB, free: 93.3 MiB)\n",
      "22/06/09 15:05:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.0.5:45668\n",
      "22/06/09 15:05:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 252 ms on 172.19.0.5 (executor 0) (1/1)\n",
      "22/06/09 15:05:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "22/06/09 15:05:27 INFO DAGScheduler: ResultStage 2 (count at cmd4.sc:1) finished in 0.277 s\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/06/09 15:05:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "22/06/09 15:05:27 INFO DAGScheduler: Job 1 finished: count at cmd4.sc:1, took 0.554218 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres4\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m29L\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33c72b7-28a5-4d33-939b-356c464b58e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 15:05:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 227281bc4555:36379 in memory (size: 6.2 KiB, free: 1111.8 MiB)\n",
      "22/06/09 15:05:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.0.5:38187 in memory (size: 6.2 KiB, free: 93.3 MiB)\n",
      "22/06/09 15:05:28 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.0.5:38187 in memory (size: 5.0 KiB, free: 93.3 MiB)\n",
      "22/06/09 15:05:28 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 227281bc4555:36379 in memory (size: 5.0 KiB, free: 1111.8 MiB)\n",
      "22/06/09 15:05:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 227281bc4555:36379 in memory (size: 4.9 KiB, free: 1111.8 MiB)\n",
      "22/06/09 15:05:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.19.0.5:38187 in memory (size: 4.9 KiB, free: 93.3 MiB)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.postgresql.util.PSQLException: ERROR: syntax error at or near \"select\"\n  Position: 15\u001b[39m\n  org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(\u001b[32mQueryExecutorImpl.java\u001b[39m:\u001b[32m2675\u001b[39m)\n  org.postgresql.core.v3.QueryExecutorImpl.processResults(\u001b[32mQueryExecutorImpl.java\u001b[39m:\u001b[32m2365\u001b[39m)\n  org.postgresql.core.v3.QueryExecutorImpl.execute(\u001b[32mQueryExecutorImpl.java\u001b[39m:\u001b[32m355\u001b[39m)\n  org.postgresql.jdbc.PgStatement.executeInternal(\u001b[32mPgStatement.java\u001b[39m:\u001b[32m490\u001b[39m)\n  org.postgresql.jdbc.PgStatement.execute(\u001b[32mPgStatement.java\u001b[39m:\u001b[32m408\u001b[39m)\n  org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(\u001b[32mPgPreparedStatement.java\u001b[39m:\u001b[32m167\u001b[39m)\n  org.postgresql.jdbc.PgPreparedStatement.executeQuery(\u001b[32mPgPreparedStatement.java\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(\u001b[32mJDBCRDD.scala\u001b[39m:\u001b[32m61\u001b[39m)\n  org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(\u001b[32mJDBCRelation.scala\u001b[39m:\u001b[32m226\u001b[39m)\n  org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(\u001b[32mJdbcRelationProvider.scala\u001b[39m:\u001b[32m35\u001b[39m)\n  org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m339\u001b[39m)\n  org.apache.spark.sql.DataFrameReader.loadV1Source(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m279\u001b[39m)\n  org.apache.spark.sql.DataFrameReader.$anonfun$load$2(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m268\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.sql.DataFrameReader.load(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m268\u001b[39m)\n  org.apache.spark.sql.DataFrameReader.load(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m203\u001b[39m)\n  org.apache.spark.sql.DataFrameReader.jdbc(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m294\u001b[39m)\n  ammonite.$sess.cmd5$Helper.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m6\u001b[39m)\n  ammonite.$sess.cmd5$.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd5$.<clinit>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val df = \n",
    "  spark\n",
    "    .read\n",
    "    .jdbc(jdbcUrl,\n",
    "         \"select * from affiliations\", \n",
    "         connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b5d3f2-006f-462b-a5e1-c46431dfad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "val queryOpts = Map(\"dbtable\"->\"(select * from affiliations where id > 10) as T\",\"lowerBound\"->\"10\",\"upperBound\"->\"20\",\"partitionColumn\"->\"id\",\"numPartitions\"->\"200\",\"url\"->jdbcUrl,\"user\"->jdbcUsername,\"password\"->jdbcPassword) \n",
    "val df =  spark.read.format(\"jdbc\").options(queryOpts).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed9092-f048-4f3f-ab61-e845411c224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec1395-bb7b-4e22-a5b3-d1927b748a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
